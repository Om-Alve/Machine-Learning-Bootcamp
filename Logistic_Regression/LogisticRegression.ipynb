{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06e1402-3551-46f1-bf60-a7e50bb11b2a",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a836638-baeb-49a4-9404-9a31ae1287e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18c493-9a4d-40f1-91d2-9fdf2c04e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_blobs(n_samples=1000,centers=2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c1f5e-d62c-4d39-a767-8eaf860c8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222543b0-f798-4732-8dcb-70fe8836a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416be33-58af-43e0-bfcc-3f4e67c9411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5b485-a747-4d64-ac3b-b4a78d137d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc02af-47f5-4b63-8016-5334905e2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26649749-1feb-44fd-9954-08ac361c2efa",
   "metadata": {},
   "source": [
    "# Custom Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11b283-edc3-45d7-a70e-ecdaceb43b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    def __init__(self,lr=0.1,iters=20):\n",
    "        self.__lr = lr\n",
    "        self.__iters= iters\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.__y = y.reshape(-1, 1)\n",
    "        X_mod = np.hstack([np.ones([len(X), 1]), X])\n",
    "        self.__X = X_mod\n",
    "        \n",
    "        self.__theta = np.zeros([X_mod.shape[1], 1])\n",
    "        \n",
    "        err = []\n",
    "        for i in range(self.__iters):\n",
    "            err.append(self.error())\n",
    "            self.gradient_accend()\n",
    "            \n",
    "        self.theta_ = self.__theta \n",
    "            \n",
    "        return err\n",
    "        \n",
    "    def gradient_accend(self):\n",
    "        delta_theta = self.gradient()\n",
    "#         print(delta_theta.shape) # (3,1)\n",
    "        self.__theta -= delta_theta\n",
    "        \n",
    "    def gradient(self):\n",
    "        yh = self.hypothesis(self.__X)\n",
    "#         print(yh.shape) # (100, 1)\n",
    "        ya = self.__y\n",
    "        diff = -(ya - yh) # (100, 1) # here i have taken -ve of log likelyhood, hence gradiet descent            \n",
    "        return np.dot(self.__X.T, diff) * self.__lr # (3,1) # for every feature, sum(diff * ith example)\n",
    "    def hypothesis(self, X):\n",
    "        prod = np.dot(X, self.__theta)\n",
    "#         print(X.shape) # (100,3)        \n",
    "#         print(self.__theta.shape) # (3,1)\n",
    "#         print(prod.shape) # (100,1)\n",
    "        return 1 / (1 + np.exp(-prod))\n",
    "        \n",
    "    def error(self):\n",
    "        yh = self.hypothesis(self.__X)\n",
    "        ya = self.__y\n",
    "        ll = ya*np.log(yh) + (1-ya)*np.log(1-yh)\n",
    "#         print(ll.shape) # (100, 1)\n",
    "        return -np.mean(ll)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_mod = np.hstack([np.ones([len(X), 1]), X])\n",
    "        yh = self.hypothesis(X_mod)\n",
    "        \n",
    "        return (yh > .5).astype(int).flatten()\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        yp = self.predict(X)\n",
    "        return np.mean(yp == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f82355-ea42-417a-8390-67422d394d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe48a43-6291-4024-95f4-8f78296fdc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
