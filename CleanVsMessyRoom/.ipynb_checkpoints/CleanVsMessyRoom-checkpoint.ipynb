{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7bc4dc6-f248-4bbd-bb42-9b9081c9d73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3e56f4e-bd84-46ac-8a93-5e2509ef54b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_messy = \"images/train/messy\"\n",
    "train_clean= \"images/train/clean\"\n",
    "test_messy= \"images/val/messy\"\n",
    "test_clean= \"images/val/clean\"\n",
    "image_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd8ce3f5-34c2-4b49-9ee6-9e0d3769d43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    train_data_messy = [] \n",
    "    train_data_clean=[]\n",
    "    for image1 in tqdm(os.listdir(train_messy)): \n",
    "        path = os.path.join(train_messy, image1)\n",
    "        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img1 = cv2.resize(img1, (image_size, image_size))\n",
    "        train_data_messy.append(img1) \n",
    "    for image2 in tqdm(os.listdir(train_clean)): \n",
    "        path = os.path.join(train_clean, image2)\n",
    "        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img2 = cv2.resize(img2, (image_size, image_size))\n",
    "        train_data_clean.append(img2) \n",
    "    \n",
    "    train_data= np.concatenate((np.asarray(train_data_messy),np.asarray(train_data_clean)),axis=0)\n",
    "    return train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9415f969-9451-45c5-8e76-216c624031bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    test_data_messy = [] \n",
    "    test_data_clean=[]\n",
    "    for image1 in tqdm(os.listdir(test_messy)): \n",
    "        path = os.path.join(test_messy, image1)\n",
    "        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img1 = cv2.resize(img1, (image_size, image_size))\n",
    "        test_data_messy.append(img1) \n",
    "    for image2 in tqdm(os.listdir(test_clean)): \n",
    "        if image2[0] == \".\":\n",
    "            continue\n",
    "        path = os.path.join(test_clean, image2)\n",
    "        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img2 = cv2.resize(img2, (image_size, image_size))\n",
    "        test_data_clean.append(img2) \n",
    "    \n",
    "    test_data= np.concatenate((np.asarray(test_data_messy),np.asarray(test_data_clean)),axis=0) \n",
    "    return test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f1b2187-2b46-4309-9796-8abefbe36707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 297.15it/s]\n",
      "100%|██████████| 96/96 [00:00<00:00, 342.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 299.06it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 270.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data() \n",
    "test_data = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67ad3071-200e-42da-83c3-7c041f876362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data=np.concatenate((train_data,test_data),axis=0)\n",
    "x_data = (x_data.astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3295d8c-069e-4e67-a117-ce845de25065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z1 = np.zeros(96)\n",
    "o1 = np.ones(96)\n",
    "Y_train = np.concatenate((o1, z1), axis=0)\n",
    "z = np.zeros(10)\n",
    "o = np.ones(10)\n",
    "Y_test = np.concatenate((o, z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9c7c273-a5c0-404e-80ae-e0de631c5564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data=np.concatenate((Y_train,Y_test),axis=0).reshape(x_data.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49acb760-352e-4497-96f8-0671509c429c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((212, 28, 28), (212, 1))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape,y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8d5f1c5-54ef-4dd7-b58a-3d48103375a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((212, 784), (212, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x_data.reshape(x_data.shape[0],x_data.shape[1]*x_data.shape[2])\n",
    "y = y_data\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93dda4d3-1c0c-44e7-9e1f-2ca3d0407da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd7f1cfd-3244-450c-8a3d-5f8d545067ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0e8b9ec-e725-4cec-b3b7-14b16b1b78b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type: linear or radial basis function\n",
    "    'gamma': [0.1, 1, 'scale'],  # Kernel coefficient (scale for 1 / (n_features * X.var()))\n",
    "}\n",
    "svc = SVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46aef30c-c116-4e66-9dc6-e7bbafacd1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;gamma&#x27;: [0.1, 1, &#x27;scale&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;gamma&#x27;: [0.1, 1, &#x27;scale&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale'],\n",
       "                         'kernel': ['linear', 'rbf']})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(svc, param_grid, cv=5)  \n",
    "grid_search.fit(X_train, y_train.ravel())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ba5d11f-aa78-4aba-b9fc-5f2795517226",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}, SVC(C=1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params,best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c2b6bc0-662c-4c9c-a0d3-d5ee3be8ec7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7285714285714285"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41605bac-097a-4e0a-804c-295ca4056a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self,ninputs,nneurons,l1_w=0,l1_b=0,l2_w=0,l2_b=0):\n",
    "        # Initialising weights and biases\n",
    "        self.weights = 0.001 * np.random.randn(ninputs,nneurons)\n",
    "        self.biases = np.zeros((1,nneurons))\n",
    "        # Regularization\n",
    "        self.l1_w = l1_w\n",
    "        self.l1_b = l1_b\n",
    "        self.l2_w = l2_w\n",
    "        self.l2_b = l2_b\n",
    "        \n",
    "    # Forward Propagation    \n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    # Backpropagation\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        \n",
    "        if self.l1_w > 0:\n",
    "            dl1w = np.ones_like(self.weights)\n",
    "            dl1w[self.weights < 0] = -1\n",
    "            self.dweights += dl1w\n",
    "        if self.l1_b > 0:\n",
    "            dl1b = np.ones_like(self.biases)\n",
    "            dl1b[self.biases < 0] = -1\n",
    "            self.dbiases += sl1b\n",
    "        if self.l2_w > 0:\n",
    "            self.dweights += self.weights * 2 * self.l2_w\n",
    "        if self.l2_b > 0:\n",
    "            self.dbiases += self.biases * 2 * self.l2_b\n",
    "            \n",
    "\n",
    "class Dropout_layer:\n",
    "    def __init__(self,drop_rate=0):\n",
    "        self.drop_rate = 1 - drop_rate\n",
    "    def forward(self,inputs,training):\n",
    "        if not training:\n",
    "            self.output = input.copy()\n",
    "            return\n",
    "        self.dropmask = np.random.binomial(1,self.drop_rate,size =inputs.shape)/(self.drop_rate)\n",
    "        self.output = inputs * self.dropmask\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.dropmask\n",
    "        \n",
    "class Activation_Relu:\n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self,inputs,training):\n",
    "        self.inputs= inputs\n",
    "        self.outputs = 1/(1+np.exp(-inputs))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.outputs * (1 - self.outputs)\n",
    "        \n",
    "class Loss_BinaryCrossentropy():\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "        (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs,y_true):\n",
    "        expvals = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True) )\n",
    "        self.output = expvals/np.sum(expvals,axis=1,keepdims=True)\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range(samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def calculate_accumulated(self,*,regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        return data_loss,self.regularization_loss()\n",
    "    \n",
    "    def calculate(self, output, y, *, regularization=False):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "        \n",
    "        return np.mean(-np.log(y_pred[range(len(y_pred)),y_true]))\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "        \n",
    "        \n",
    "class Activation_softmax_cross_entropy:\n",
    "#     def __init__(self):\n",
    "#         self.activation = Activation_Softmax()\n",
    "#         self.lossfunc = CrossEntropyLoss()\n",
    "    \n",
    "#     def forward(self,inputs,y_true):\n",
    "#         self.activation.forward(inputs,y_true)\n",
    "#         self.output = self.activation.output\n",
    "#         return self.lossfunc.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(y_true)\n",
    "        \n",
    "        # Turning one hot encoded arrays to sparse vectors\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        \n",
    "        self.dinputs/=samples\n",
    "        \n",
    "        \n",
    "class Adam_Optimizer:\n",
    "    def __init__(self,lr=0.001,decay_rate=0,epsilon= 1e-7,beta1=0.9,beta2=0.999):\n",
    "        self.initiallr = lr\n",
    "        self.currentlr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "    \n",
    "    def pre_update(self):\n",
    "        self.currentlr = self.initiallr * (1/(1+(self.decay_rate * self.iterations)))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentum = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_momentum = self.beta1 * layer.weight_momentum + (1-self.beta1) * layer.dweights\n",
    "        layer.bias_momentum = self.beta1 * layer.bias_momentum + (1 - self.beta1) * layer.dbiases\n",
    "        \n",
    "        layer.weight_momentum_prime = layer.weight_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        layer.bias_momentum_prime = layer.bias_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = layer.weight_cache * self.beta2 + (1-self.beta2) * (layer.dweights ** 2)\n",
    "        layer.bias_cache = layer.bias_cache * self.beta2 + (1-self.beta2) * (layer.dbiases ** 2)\n",
    "        \n",
    "        layer.weight_cache_prime = layer.weight_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "        layer.bias_cache_prime = layer.bias_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += - self.currentlr * layer.weight_momentum_prime  / (np.sqrt(layer.weight_cache_prime) + self.epsilon)\n",
    "        layer.biases += -self.currentlr * layer.bias_momentum_prime / (np.sqrt(layer.bias_cache_prime) + self.epsilon)\n",
    "        \n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "    \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self,*,loss,optimizer,accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "    \n",
    "    def train(self,X,y,*,epochs,print_every,validation_data = None,batch_size = None):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "            X_val,y_val = validation_data\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "                \n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            \n",
    "            print(\"Epoch : \",epoch)\n",
    "            \n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size : (step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size : (step+1) * batch_size]\n",
    "\n",
    "                    \n",
    "                output = self.forward(batch_X,training = True)\n",
    "                data_loss,regularization_loss = self.loss.calculate(output,batch_y,regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions,batch_y)\n",
    "\n",
    "                self.backward(output,batch_y)\n",
    "\n",
    "                self.optimizer.pre_update()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'iteration: {step}, ' +\n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss: {loss:.3f} (' +\n",
    "                    f'data_loss: {data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "            \n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            \n",
    "            print(f'training, ' +\n",
    "                    f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                    f'loss: {epoch_loss:.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "\n",
    "            \n",
    "            \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.accuracy.new_pass()\n",
    "            self.loss.new_pass()\n",
    "            \n",
    "            for step in range(validation_steps):\n",
    "                if batch_size is None:\n",
    "                    batch_X = X_val\n",
    "                    batch_y = y_val\n",
    "                else:\n",
    "                    batch_X = X_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    batch_y = y_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    \n",
    "            \n",
    "            output = self.forward(batch_X,training=False)\n",
    "            loss = self.loss.calculate(output, batch_y)\n",
    "            predictions = self.output_activation.predictions(\n",
    "            output)\n",
    "            accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "            validation_loss = self.loss.calculate_accumulated()\n",
    "            print(f'validation, ' +\n",
    "            f'acc: {validation_accuracy:.3f}, ' +\n",
    "            f'loss: {validation_loss:.3f}')\n",
    "\n",
    "        \n",
    "    def finalize(self):\n",
    "        self.input_layer = Input_Layer()\n",
    "        self.trainable_layers = []\n",
    "        nlayers = len(self.layers)\n",
    "        \n",
    "        for i in range(nlayers):\n",
    "            if i==0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            elif i < nlayers - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_activation = self.layers[i]\n",
    "            \n",
    "            if hasattr(self.layers[i],\"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)   \n",
    "        \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "        isinstance(self.loss, CrossEntropyLoss):\n",
    "            self.softmax_classifier_output = \\\n",
    "            Activation_softmax_cross_entropy()\n",
    "\n",
    "            \n",
    "    def forward(self,X,training):\n",
    "        self.input_layer.forward(X)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output,training)\n",
    "        return layer.output\n",
    "    def backward(self,output,y):\n",
    "        \n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = \\\n",
    "            self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "        \n",
    "        \n",
    "        self.loss.backward(output,y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "        \n",
    "class Input_Layer:\n",
    "    def forward(self,inputs):\n",
    "        self.output = inputs\n",
    "        \n",
    "class Accuracy:\n",
    "    def calculate(self,predictions,y):\n",
    "        comparisions = self.compare(predictions,y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "        self.accumulated_sum += accuracy\n",
    "        self.accumulated_count += 1\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_count = 0\n",
    "        self.accumulated_sum = 0\n",
    "    \n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "class Accuracy_Classification(Accuracy):\n",
    "    def init(self,y):\n",
    "        pass\n",
    "    def compare(self,predictions,y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y,axis=1)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbfc5f34-c165-4a14-9e32-adfa05a28ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "iteration: 0, acc: 0.542, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.542, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  2\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  3\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  4\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  5\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  6\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  7\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  8\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  9\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  10\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "validation, acc: 1.000, loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_10484\\1884628948.py:150: RuntimeWarning: divide by zero encountered in log\n",
      "  negative_log_likelihoods = -np.log(correct_confidences)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Dense_Layer(784,64))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(64,64))\n",
    "# model.add(Activation_Relu())\n",
    "# model.add(Dense_Layer(64,64))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(64,2))\n",
    "model.add(Activation_Softmax())\n",
    "model.set(loss=CrossEntropyLoss(),accuracy=Accuracy_Classification(),optimizer=Adam_Optimizer(decay_rate=1e-5))\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X_train,y_train,epochs=10,print_every=10,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59c85128-4668-45a4-b95c-93ffb9f559fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = model.forward(X_test,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "735d6a91-d070-4936-abd4-789c2d7ef894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred =  np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d560740b-29d7-4a1f-b596-d898dc06f41f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
