{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bc4dc6-f248-4bbd-bb42-9b9081c9d73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e56f4e-bd84-46ac-8a93-5e2509ef54b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_messy = \"images/train/messy\"\n",
    "train_clean= \"images/train/clean\"\n",
    "test_messy= \"images/val/messy\"\n",
    "test_clean= \"images/val/clean\"\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8ce3f5-34c2-4b49-9ee6-9e0d3769d43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    train_data_messy = [] \n",
    "    train_data_clean=[]\n",
    "    for image1 in tqdm(os.listdir(train_messy)): \n",
    "        path = os.path.join(train_messy, image1)\n",
    "        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img1 = cv2.resize(img1, (image_size, image_size))\n",
    "        train_data_messy.append(img1) \n",
    "    for image2 in tqdm(os.listdir(train_clean)): \n",
    "        path = os.path.join(train_clean, image2)\n",
    "        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img2 = cv2.resize(img2, (image_size, image_size))\n",
    "        train_data_clean.append(img2) \n",
    "    \n",
    "    train_data= np.concatenate((np.asarray(train_data_messy),np.asarray(train_data_clean)),axis=0)\n",
    "    return train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9415f969-9451-45c5-8e76-216c624031bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    test_data_messy = [] \n",
    "    test_data_clean=[]\n",
    "    for image1 in tqdm(os.listdir(test_messy)): \n",
    "        path = os.path.join(test_messy, image1)\n",
    "        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img1 = cv2.resize(img1, (image_size, image_size))\n",
    "        test_data_messy.append(img1) \n",
    "    for image2 in tqdm(os.listdir(test_clean)): \n",
    "        if image2[0] == \".\":\n",
    "            continue\n",
    "        path = os.path.join(test_clean, image2)\n",
    "        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "        img2 = cv2.resize(img2, (image_size, image_size))\n",
    "        test_data_clean.append(img2) \n",
    "    \n",
    "    test_data= np.concatenate((np.asarray(test_data_messy),np.asarray(test_data_clean)),axis=0) \n",
    "    return test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1b2187-2b46-4309-9796-8abefbe36707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 138.40it/s]\n",
      "100%|██████████| 96/96 [00:00<00:00, 189.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 164.60it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 168.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data() \n",
    "test_data = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ad3071-200e-42da-83c3-7c041f876362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data=np.concatenate((train_data,test_data),axis=0)\n",
    "x_data = (x_data.astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3295d8c-069e-4e67-a117-ce845de25065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z1 = np.zeros(96)\n",
    "o1 = np.ones(96)\n",
    "Y_train = np.concatenate((o1, z1), axis=0)\n",
    "z = np.zeros(10)\n",
    "o = np.ones(10)\n",
    "Y_test = np.concatenate((o, z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c7c273-a5c0-404e-80ae-e0de631c5564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data=np.concatenate((Y_train,Y_test),axis=0).reshape(x_data.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49acb760-352e-4497-96f8-0671509c429c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((212, 128, 128), (212, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape,y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d5f1c5-54ef-4dd7-b58a-3d48103375a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((212, 16384), (212, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x_data.reshape(x_data.shape[0],x_data.shape[1]*x_data.shape[2])\n",
    "y = y_data\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dda4d3-1c0c-44e7-9e1f-2ca3d0407da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd7f1cfd-3244-450c-8a3d-5f8d545067ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e8b9ec-e725-4cec-b3b7-14b16b1b78b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type: linear or radial basis function\n",
    "    'gamma': [0.1, 1, 'scale'],  # Kernel coefficient (scale for 1 / (n_features * X.var()))\n",
    "}\n",
    "svc = SVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46aef30c-c116-4e66-9dc6-e7bbafacd1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(svc, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \n\u001b[1;32m----> 2\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train\u001b[38;5;241m.\u001b[39mravel())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m libsvm\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    330\u001b[0m     X,\n\u001b[0;32m    331\u001b[0m     y,\n\u001b[0;32m    332\u001b[0m     svm_type\u001b[38;5;241m=\u001b[39msolver_type,\n\u001b[0;32m    333\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    336\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[0;32m    337\u001b[0m     C\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m    338\u001b[0m     nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu,\n\u001b[0;32m    339\u001b[0m     probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability,\n\u001b[0;32m    340\u001b[0m     degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree,\n\u001b[0;32m    341\u001b[0m     shrinking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinking,\n\u001b[0;32m    342\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    343\u001b[0m     cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size,\n\u001b[0;32m    344\u001b[0m     coef0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0,\n\u001b[0;32m    345\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma,\n\u001b[0;32m    346\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon,\n\u001b[0;32m    347\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m    348\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[0;32m    349\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(svc, param_grid, cv=5)  \n",
    "grid_search.fit(X_train, y_train.ravel())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5d11f-aa78-4aba-b9fc-5f2795517226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params,best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c2b6bc0-662c-4c9c-a0d3-d5ee3be8ec7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7285714285714285"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41605bac-097a-4e0a-804c-295ca4056a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self,ninputs,nneurons,l1_w=0,l1_b=0,l2_w=0,l2_b=0):\n",
    "        # Initialising weights and biases\n",
    "        self.weights = 0.001 * np.random.randn(ninputs,nneurons)\n",
    "        self.biases = np.zeros((1,nneurons))\n",
    "        # Regularization\n",
    "        self.l1_w = l1_w\n",
    "        self.l1_b = l1_b\n",
    "        self.l2_w = l2_w\n",
    "        self.l2_b = l2_b\n",
    "        \n",
    "    # Forward Propagation    \n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    # Backpropagation\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        \n",
    "        if self.l1_w > 0:\n",
    "            dl1w = np.ones_like(self.weights)\n",
    "            dl1w[self.weights < 0] = -1\n",
    "            self.dweights += dl1w\n",
    "        if self.l1_b > 0:\n",
    "            dl1b = np.ones_like(self.biases)\n",
    "            dl1b[self.biases < 0] = -1\n",
    "            self.dbiases += sl1b\n",
    "        if self.l2_w > 0:\n",
    "            self.dweights += self.weights * 2 * self.l2_w\n",
    "        if self.l2_b > 0:\n",
    "            self.dbiases += self.biases * 2 * self.l2_b\n",
    "            \n",
    "\n",
    "class Dropout_layer:\n",
    "    def __init__(self,drop_rate=0):\n",
    "        self.drop_rate = 1 - drop_rate\n",
    "    def forward(self,inputs,training):\n",
    "        if not training:\n",
    "            self.output = input.copy()\n",
    "            return\n",
    "        self.dropmask = np.random.binomial(1,self.drop_rate,size =inputs.shape)/(self.drop_rate)\n",
    "        self.output = inputs * self.dropmask\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.dropmask\n",
    "        \n",
    "class Activation_Relu:\n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self,inputs,training):\n",
    "        self.inputs= inputs\n",
    "        self.outputs = 1/(1+np.exp(-inputs))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.outputs * (1 - self.outputs)\n",
    "        \n",
    "class Loss_BinaryCrossentropy():\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "        (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs,y_true):\n",
    "        expvals = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True) )\n",
    "        self.output = expvals/np.sum(expvals,axis=1,keepdims=True)\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range(samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def calculate_accumulated(self,*,regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        return data_loss,self.regularization_loss()\n",
    "    \n",
    "    def calculate(self, output, y, *, regularization=False):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "        \n",
    "        return np.mean(-np.log(y_pred[range(len(y_pred)),y_true]))\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "        \n",
    "        \n",
    "class Activation_softmax_cross_entropy:\n",
    "#     def __init__(self):\n",
    "#         self.activation = Activation_Softmax()\n",
    "#         self.lossfunc = CrossEntropyLoss()\n",
    "    \n",
    "#     def forward(self,inputs,y_true):\n",
    "#         self.activation.forward(inputs,y_true)\n",
    "#         self.output = self.activation.output\n",
    "#         return self.lossfunc.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(y_true)\n",
    "        \n",
    "        # Turning one hot encoded arrays to sparse vectors\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        \n",
    "        self.dinputs/=samples\n",
    "        \n",
    "        \n",
    "class Adam_Optimizer:\n",
    "    def __init__(self,lr=0.001,decay_rate=0,epsilon= 1e-7,beta1=0.9,beta2=0.999):\n",
    "        self.initiallr = lr\n",
    "        self.currentlr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "    \n",
    "    def pre_update(self):\n",
    "        self.currentlr = self.initiallr * (1/(1+(self.decay_rate * self.iterations)))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentum = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_momentum = self.beta1 * layer.weight_momentum + (1-self.beta1) * layer.dweights\n",
    "        layer.bias_momentum = self.beta1 * layer.bias_momentum + (1 - self.beta1) * layer.dbiases\n",
    "        \n",
    "        layer.weight_momentum_prime = layer.weight_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        layer.bias_momentum_prime = layer.bias_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = layer.weight_cache * self.beta2 + (1-self.beta2) * (layer.dweights ** 2)\n",
    "        layer.bias_cache = layer.bias_cache * self.beta2 + (1-self.beta2) * (layer.dbiases ** 2)\n",
    "        \n",
    "        layer.weight_cache_prime = layer.weight_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "        layer.bias_cache_prime = layer.bias_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += - self.currentlr * layer.weight_momentum_prime  / (np.sqrt(layer.weight_cache_prime) + self.epsilon)\n",
    "        layer.biases += -self.currentlr * layer.bias_momentum_prime / (np.sqrt(layer.bias_cache_prime) + self.epsilon)\n",
    "        \n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "    \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self,*,loss,optimizer,accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "    \n",
    "    def train(self,X,y,*,epochs,print_every,validation_data = None,batch_size = None):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "            X_val,y_val = validation_data\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "                \n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            \n",
    "            print(\"Epoch : \",epoch)\n",
    "            \n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size : (step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size : (step+1) * batch_size]\n",
    "\n",
    "                    \n",
    "                output = self.forward(batch_X,training = True)\n",
    "                data_loss,regularization_loss = self.loss.calculate(output,batch_y,regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions,batch_y)\n",
    "\n",
    "                self.backward(output,batch_y)\n",
    "\n",
    "                self.optimizer.pre_update()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'iteration: {step}, ' +\n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss: {loss:.3f} (' +\n",
    "                    f'data_loss: {data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "            \n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            \n",
    "            print(f'training, ' +\n",
    "                    f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                    f'loss: {epoch_loss:.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "\n",
    "            \n",
    "            \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.accuracy.new_pass()\n",
    "            self.loss.new_pass()\n",
    "            \n",
    "            for step in range(validation_steps):\n",
    "                if batch_size is None:\n",
    "                    batch_X = X_val\n",
    "                    batch_y = y_val\n",
    "                else:\n",
    "                    batch_X = X_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    batch_y = y_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    \n",
    "            \n",
    "            output = self.forward(batch_X,training=False)\n",
    "            loss = self.loss.calculate(output, batch_y)\n",
    "            predictions = self.output_activation.predictions(\n",
    "            output)\n",
    "            accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "            validation_loss = self.loss.calculate_accumulated()\n",
    "            print(f'validation, ' +\n",
    "            f'acc: {validation_accuracy:.3f}, ' +\n",
    "            f'loss: {validation_loss:.3f}')\n",
    "\n",
    "        \n",
    "    def finalize(self):\n",
    "        self.input_layer = Input_Layer()\n",
    "        self.trainable_layers = []\n",
    "        nlayers = len(self.layers)\n",
    "        \n",
    "        for i in range(nlayers):\n",
    "            if i==0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            elif i < nlayers - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_activation = self.layers[i]\n",
    "            \n",
    "            if hasattr(self.layers[i],\"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)   \n",
    "        \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "        isinstance(self.loss, CrossEntropyLoss):\n",
    "            self.softmax_classifier_output = \\\n",
    "            Activation_softmax_cross_entropy()\n",
    "\n",
    "            \n",
    "    def forward(self,X,training):\n",
    "        self.input_layer.forward(X)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output,training)\n",
    "        return layer.output\n",
    "    def backward(self,output,y):\n",
    "        \n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = \\\n",
    "            self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "        \n",
    "        \n",
    "        self.loss.backward(output,y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "        \n",
    "class Input_Layer:\n",
    "    def forward(self,inputs):\n",
    "        self.output = inputs\n",
    "        \n",
    "class Accuracy:\n",
    "    def calculate(self,predictions,y):\n",
    "        comparisions = self.compare(predictions,y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "        self.accumulated_sum += accuracy\n",
    "        self.accumulated_count += 1\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_count = 0\n",
    "        self.accumulated_sum = 0\n",
    "    \n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "class Accuracy_Classification(Accuracy):\n",
    "    def init(self,y):\n",
    "        pass\n",
    "    def compare(self,predictions,y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y,axis=1)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cbfc5f34-c165-4a14-9e32-adfa05a28ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "iteration: 0, acc: 0.028, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.028, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  2\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  3\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  4\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  5\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  6\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  7\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  8\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  9\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  10\n",
      "iteration: 0, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 1.000, loss: inf (data_loss: inf, reg_loss: 0.000), lr: 0.001\n",
      "validation, acc: 1.000, loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_10484\\1884628948.py:150: RuntimeWarning: divide by zero encountered in log\n",
      "  negative_log_likelihoods = -np.log(correct_confidences)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Dense_Layer(784,64))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(64,64))\n",
    "# model.add(Activation_Relu())\n",
    "# model.add(Dense_Layer(64,64))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(64,2))\n",
    "model.add(Activation_Softmax())\n",
    "model.set(loss=CrossEntropyLoss(),accuracy=Accuracy_Classification(),optimizer=Adam_Optimizer(decay_rate=1e-5))\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X_train,y_train,epochs=10,print_every=10,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59c85128-4668-45a4-b95c-93ffb9f559fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = model.forward(X_test,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "735d6a91-d070-4936-abd4-789c2d7ef894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred =  np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d560740b-29d7-4a1f-b596-d898dc06f41f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
