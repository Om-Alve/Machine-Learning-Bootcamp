{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNfBMH8B_Xk"
      },
      "source": [
        "# Handling multiple sequences (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMcLFP2wB_Xo"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uujgoefjB_Xq",
        "outputId": "c5c06000-9225-42e3-cfc7-45141f86f75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276216,  2.8789377]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R_sYT3R8B_Xs",
        "outputId": "3c608686-9947-42bf-eb3f-84a034914cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
            "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8SfQzkFlB_Xs",
        "outputId": "6f7e3ef9-c4c4-42eb-cfe0-63910311acdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tf.Tensor(\n",
            "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
            "   2166  1012]], shape=(1, 14), dtype=int32)\n",
            "Logits: tf.Tensor([[-2.7276216  2.8789377]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = tf.constant([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BW2uirlxB_Xt"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5LpEftqkB_Xt"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7X6JyThjB_Xu",
        "outputId": "5011033d-81c8-4df2-afe4-8c0c1e2fa775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[ 1.5693669 -1.3894575]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 0.58030236 -0.41252634]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.5693674 -1.3894581]\n",
            " [ 1.3373474 -1.2163182]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(tf.constant(sequence1_ids)).logits)\n",
        "print(model(tf.constant(sequence2_ids)).logits)\n",
        "print(model(tf.constant(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dhk6KmDMB_Xu",
        "outputId": "3f46835e-4da5-474b-8461-bb0acb1ab9b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 1.5693674 -1.3894581]\n",
            " [ 0.5802975 -0.4125216]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y_SrcsPDB_Xv",
        "outputId": "f4897a34-63b2-4ca2-9d93-4fe864541536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sequence ['i', 'hate', 'this', 'so', 'much', '!']\n",
            "Input Sequence tf.Tensor([[1045 5223 2023 2061 2172  999]], shape=(1, 6), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "sequence = \"I hate this so much!\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_seq = tokenizer.tokenize(sequence)\n",
        "print(\"Tokenized Sequence\",tokenized_seq)\n",
        "input_seq = tokenizer.convert_tokens_to_ids(tokenized_seq)\n",
        "input_seq = tf.constant([input_seq])\n",
        "print(\"Input Sequence\",input_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "output = model(input_seq).logits\n",
        "print(\"Logits: \",output)\n",
        "print(\"Prediction: \",tf.math.softmax(output,axis=-1))"
      ],
      "metadata": {
        "id": "hQow8Yu-CDVT",
        "outputId": "641d0f2f-7814-4070-f977-989f0c6518c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits:  tf.Tensor([[ 3.193092  -2.6685228]], shape=(1, 2), dtype=float32)\n",
            "Prediction:  tf.Tensor([[0.9971615  0.00283856]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VabT1Xt0GLK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (TensorFlow)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}