{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rOtRK75Bb8u"
      },
      "source": [
        "# Tokenizers (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOBT2YbNBb8w"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9lYw8yDLBb8x",
        "outputId": "4268b06d-c86d-45dd-ec1f-535961c551da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', 'this', 'is', 'Om', 'Alve!']\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = \"Hello world this is Om Alve!\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JSTdyBbvBb8y"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RdCr5T-kBb8y"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_r0zdwJDBb8y",
        "outputId": "25165488-46ca-4489-d949-c19d6f280d53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'n',\n",
              " '##l',\n",
              " '##p',\n",
              " 'from',\n",
              " 'hugging',\n",
              " 'face',\n",
              " 'and',\n",
              " 'it',\n",
              " \"'\",\n",
              " 's',\n",
              " 'really',\n",
              " 'interesting']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer.tokenize(\"I am learning nlp from hugging face and it's really interesting\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KAEs-LhbBb8z",
        "outputId": "0c60a9f3-94b9-4f44-d26d-42fcee91b42a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'learning', 'n', '##l', '##p', 'from', 'hugging', 'face', 'and', 'it', \"'\", 's', 'really', 'interesting']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"I am learning nlp from hugging face and it's really interesting\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FuNZmBUSBb8z",
        "outputId": "25b40798-2c3b-437d-abd8-70b5ec81854c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146, 1821, 3776, 183, 1233, 1643, 1121, 19558, 1339, 1105, 1122, 112, 188, 1541, 5426]\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bhslwAbRBb8z",
        "outputId": "7c45b1d9-c253-4e31-90d9-fbde95b4791c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am learning nlp from hugging face and it's really interesting\n"
          ]
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([146, 1821, 3776, 183, 1233, 1643, 1121, 19558, 1339, 1105, 1122, 112, 188, 1541, 5426])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "053Gx5VdCqIM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (TensorFlow)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}