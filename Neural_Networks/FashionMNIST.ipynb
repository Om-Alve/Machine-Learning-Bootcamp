{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62679da6-ddc4-4c52-a1f1-a86deb6cc345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d5113020-e73a-4a27-88c5-705c6a4e17d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1968003f-536f-4232-84f5-4e59cb5702c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000.png', '0001.png', '0002.png', '0003.png', '0004.png', '0005.png', '0006.png', '0007.png', '0008.png', '0009.png']\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('fashion_mnist_images/train/0')\n",
    "print(files[:10])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "effcd2e3-541b-450f-b18c-ed17a9a1a2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0  51   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 139 214 218 220 164 206 243 233 205  93   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 130 253 225 226 233 229 232 230 219 227 249  63   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 203 237 221 222 221 222 219 220 224 218 233 191   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 232 237 224 225 224 224 222 221 225 218 224 253   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 255 232 223 225 222 221 219 216 219 212 223 255  30   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   5 255 230 224 221 223 218 219 217 221 214 229 255  89   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  32 255 228 221 220 223 221 221 218 217 221 232 255 113   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  78 255 227 218 220 221 226 225 219 215 232 168 255 148   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 115 255 237 221 221 218 228 227 219 216 241 107 255 152   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 144 255 218 223 220 215 223 222 215 216 240 119 255 154   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 167 255 102 224 223 215 219 220 213 213 234 131 255 165   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 170 255  34 221 229 215 217 217 214 216 238 102 254 175   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 172 255  27 235 225 215 214 215 215 213 246 104 253 181   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 173 255  18 249 217 215 215 215 216 210 241  95 247 183   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 172 255  19 252 214 216 215 214 215 211 245 106 244 176   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 164 254  27 253 212 217 216 214 215 214 243 110 243 145   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 169 255  42 253 211 215 218 218 215 215 233 149 255 141   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 103 131  49 253 212 216 222 219 217 214 249 128 122  78   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  58 254 218 217 225 218 219 212 253 110   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   4   0  64 237 219 220 229 217 222 217 235 129   0   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  54 239 221 222 231 215 225 217 237 125   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  50 241 220 224 233 212 227 217 241 120   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  41 242 222 226 236 213 228 220 243 113   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  33 242 224 228 239 216 230 221 245  97   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  13 237 224 226 235 208 226 218 246  65   0   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0 217 244 245 255 253 241 236 248  22   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 115 181 103  54 141 146 134 101   0   0   1   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)\n",
    "import cv2\n",
    "image_data = cv2.imread('fashion_mnist_images/train/3/0002.png',\n",
    "cv2.IMREAD_UNCHANGED)\n",
    "print(image_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ca6cbc82-ef7b-4c77-a9b0-b17f77b6db46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2674506cd50>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "919bafd6-b8da-44d3-a061-d091dd49dd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = os.listdir('fashion_mnist_images/train/')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec70b22d-d4db-491b-823e-7516e232e5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(dataset,path):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label in tqdm(labels):\n",
    "        for file in os.listdir(os.path.join(path,dataset,label)):\n",
    "            if(file[0] == '.'):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(path,dataset,label,file),cv2.IMREAD_UNCHANGED)\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).astype('uint8')\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e0a926e5-0a0f-481c-a696-1232ba49a46e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"fashion_mnist_images/\"\n",
    "\n",
    "X,y = load_data(\"train\",PATH)\n",
    "X_test,y_test = load_data(\"test\",PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333db112-6606-49c0-9958-57fa70076166",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bb8491ca-84e3-43f5-98b6-cef135759cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling the data between -1 and 1\n",
    "\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "496c1054-3360-4649-add0-dee0bfb0e82f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(),X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d374a650-fa55-43ae-921b-f20f6032db17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (60000, 28, 28)  X_test shape :  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape : \",X.shape,\" X_test shape : \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c1d0e7b8-9c6b-4750-bd9d-7160dd00f912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (60000, 784)  X_test shape :  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Converting the 2D array of features into a 1D array\n",
    "X = X.reshape(X.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "print(\"X shape : \",X.shape,\" X_test shape : \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d7c482c0-c4cd-4cca-b802-401c66acaf05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "\n",
    "idxs = np.array(range(X.shape[0]))\n",
    "\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "X = X[idxs]\n",
    "y = y[idxs]\n",
    "\n",
    "test_idxs = np.array(range(X_test.shape[0]))\n",
    "np.random.shuffle(test_idxs)\n",
    "\n",
    "X_test = X_test[test_idxs]\n",
    "y_test = y_test[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d7094c3b-a40b-4b0b-924d-bb455482c4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 0, 2, 7, 8, 0, 3, 5, 0], dtype=uint8)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "39bb7359-fb5c-405e-b2f1-ce26fe9dc8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self,ninputs,nneurons,l1_w=0,l1_b=0,l2_w=0,l2_b=0):\n",
    "        # Initialising weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(ninputs,nneurons)\n",
    "        self.biases = np.zeros((1,nneurons))\n",
    "        # Regularization\n",
    "        self.l1_w = l1_w\n",
    "        self.l1_b = l1_b\n",
    "        self.l2_w = l2_w\n",
    "        self.l2_b = l2_b\n",
    "        \n",
    "    # Forward Propagation    \n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    # Backpropagation\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        \n",
    "        if self.l1_w > 0:\n",
    "            dl1w = np.ones_like(self.weights)\n",
    "            dl1w[self.weights < 0] = -1\n",
    "            self.dweights += dl1w\n",
    "        if self.l1_b > 0:\n",
    "            dl1b = np.ones_like(self.biases)\n",
    "            dl1b[self.biases < 0] = -1\n",
    "            self.dbiases += sl1b\n",
    "        if self.l2_w > 0:\n",
    "            self.dweights += self.weights * 2 * self.l2_w\n",
    "        if self.l2_b > 0:\n",
    "            self.dbiases += self.biases * 2 * self.l2_b\n",
    "            \n",
    "\n",
    "class Dropout_layer:\n",
    "    def __init__(self,drop_rate=0):\n",
    "        self.drop_rate = 1 - drop_rate\n",
    "    def forward(self,inputs,training):\n",
    "        if not training:\n",
    "            self.output = input.copy()\n",
    "            return\n",
    "        self.dropmask = np.random.binomial(1,self.drop_rate,size =inputs.shape)/(self.drop_rate)\n",
    "        self.output = inputs * self.dropmask\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.dropmask\n",
    "        \n",
    "class Activation_Relu:\n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self,inputs,training):\n",
    "        self.inputs= inputs\n",
    "        self.outputs = 1/(1+np.exp(-inputs))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.outputs * (1 - self.outputs)\n",
    "        \n",
    "class Loss_BinaryCrossentropy():\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "        (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs,y_true):\n",
    "        expvals = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True) )\n",
    "        self.output = expvals/np.sum(expvals,axis=1,keepdims=True)\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range(samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def calculate_accumulated(self,*,regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        return data_loss,self.regularization_loss()\n",
    "    \n",
    "    def calculate(self, output, y, *, regularization=False):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "        \n",
    "        return np.mean(-np.log(y_pred[range(len(y_pred)),y_true]))\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "        \n",
    "        \n",
    "class Activation_softmax_cross_entropy:\n",
    "#     def __init__(self):\n",
    "#         self.activation = Activation_Softmax()\n",
    "#         self.lossfunc = CrossEntropyLoss()\n",
    "    \n",
    "#     def forward(self,inputs,y_true):\n",
    "#         self.activation.forward(inputs,y_true)\n",
    "#         self.output = self.activation.output\n",
    "#         return self.lossfunc.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(y_true)\n",
    "        \n",
    "        # Turning one hot encoded arrays to sparse vectors\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        \n",
    "        self.dinputs/=samples\n",
    "        \n",
    "        \n",
    "class Adam_Optimizer:\n",
    "    def __init__(self,lr=0.001,decay_rate=0,epsilon= 1e-7,beta1=0.9,beta2=0.999):\n",
    "        self.initiallr = lr\n",
    "        self.currentlr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "    \n",
    "    def pre_update(self):\n",
    "        self.currentlr = self.initiallr * (1/(1+(self.decay_rate * self.iterations)))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentum = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_momentum = self.beta1 * layer.weight_momentum + (1-self.beta1) * layer.dweights\n",
    "        layer.bias_momentum = self.beta1 * layer.bias_momentum + (1 - self.beta1) * layer.dbiases\n",
    "        \n",
    "        layer.weight_momentum_prime = layer.weight_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        layer.bias_momentum_prime = layer.bias_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = layer.weight_cache * self.beta2 + (1-self.beta2) * (layer.dweights ** 2)\n",
    "        layer.bias_cache = layer.bias_cache * self.beta2 + (1-self.beta2) * (layer.dbiases ** 2)\n",
    "        \n",
    "        layer.weight_cache_prime = layer.weight_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "        layer.bias_cache_prime = layer.bias_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += - self.currentlr * layer.weight_momentum_prime  / (np.sqrt(layer.weight_cache_prime) + self.epsilon)\n",
    "        layer.biases += -self.currentlr * layer.bias_momentum_prime / (np.sqrt(layer.bias_cache_prime) + self.epsilon)\n",
    "        \n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "    \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self,*,loss,optimizer,accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "    \n",
    "    def train(self,X,y,*,epochs,print_every,validation_data = None,batch_size = None):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "            X_val,y_val = validation_data\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "                \n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            \n",
    "            print(\"Epoch : \",epoch)\n",
    "            \n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size : (step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size : (step+1) * batch_size]\n",
    "\n",
    "                    \n",
    "                output = self.forward(batch_X,training = True)\n",
    "                data_loss,regularization_loss = self.loss.calculate(output,batch_y,regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions,batch_y)\n",
    "\n",
    "                self.backward(output,batch_y)\n",
    "\n",
    "                self.optimizer.pre_update()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'iteration: {step}, ' +\n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss: {loss:.3f} (' +\n",
    "                    f'data_loss: {data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "            \n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            \n",
    "            print(f'training, ' +\n",
    "                    f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                    f'loss: {epoch_loss:.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "\n",
    "            \n",
    "            \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.accuracy.new_pass()\n",
    "            self.loss.new_pass()\n",
    "            \n",
    "            for step in range(validation_steps):\n",
    "                if batch_size is None:\n",
    "                    batch_X = X_val\n",
    "                    batch_y = y_val\n",
    "                else:\n",
    "                    batch_X = X_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    batch_y = y_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    \n",
    "            \n",
    "            output = self.forward(batch_X,training=False)\n",
    "            loss = self.loss.calculate(output, batch_y)\n",
    "            predictions = self.output_activation.predictions(\n",
    "            output)\n",
    "            accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "            validation_loss = self.loss.calculate_accumulated()\n",
    "            print(f'validation, ' +\n",
    "            f'acc: {validation_accuracy:.3f}, ' +\n",
    "            f'loss: {validation_loss:.3f}')\n",
    "\n",
    "        \n",
    "    def finalize(self):\n",
    "        self.input_layer = Input_Layer()\n",
    "        self.trainable_layers = []\n",
    "        nlayers = len(self.layers)\n",
    "        \n",
    "        for i in range(nlayers):\n",
    "            if i==0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            elif i < nlayers - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_activation = self.layers[i]\n",
    "            \n",
    "            if hasattr(self.layers[i],\"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)   \n",
    "        \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "        isinstance(self.loss, CrossEntropyLoss):\n",
    "            self.softmax_classifier_output = \\\n",
    "            Activation_softmax_cross_entropy()\n",
    "\n",
    "            \n",
    "    def forward(self,X,training):\n",
    "        self.input_layer.forward(X)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output,training)\n",
    "        return layer.output\n",
    "    def backward(self,output,y):\n",
    "        \n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = \\\n",
    "            self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "        \n",
    "        \n",
    "        self.loss.backward(output,y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "        \n",
    "class Input_Layer:\n",
    "    def forward(self,inputs):\n",
    "        self.output = inputs\n",
    "        \n",
    "class Accuracy:\n",
    "    def calculate(self,predictions,y):\n",
    "        comparisions = self.compare(predictions,y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "        self.accumulated_sum += accuracy\n",
    "        self.accumulated_count += 1\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_count = 0\n",
    "        self.accumulated_sum = 0\n",
    "    \n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "class Accuracy_Classification(Accuracy):\n",
    "    def init(self,y):\n",
    "        pass\n",
    "    def compare(self,predictions,y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y,axis=1)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b31e6ab7-5c23-48ee-b16d-560498857e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "iteration: 0, acc: 0.078, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.773, loss: 0.549 (data_loss: 0.549, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.812, loss: 0.537 (data_loss: 0.537, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.820, loss: 0.521 (data_loss: 0.521, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.828, loss: 0.368 (data_loss: 0.368, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.885, loss: 0.298 (data_loss: 0.298, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.794, loss: 0.555 (data_loss: 0.555, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  2\n",
      "iteration: 0, acc: 0.844, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.914, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.852, loss: 0.408 (data_loss: 0.408, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.852, loss: 0.449 (data_loss: 0.449, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.906, loss: 0.269 (data_loss: 0.269, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.896, loss: 0.244 (data_loss: 0.244, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.856, loss: 0.390 (data_loss: 0.390, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  3\n",
      "iteration: 0, acc: 0.875, loss: 0.375 (data_loss: 0.375, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.930, loss: 0.253 (data_loss: 0.253, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.875, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.867, loss: 0.410 (data_loss: 0.410, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.922, loss: 0.229 (data_loss: 0.229, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.906, loss: 0.218 (data_loss: 0.218, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.872, loss: 0.350 (data_loss: 0.350, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  4\n",
      "iteration: 0, acc: 0.883, loss: 0.362 (data_loss: 0.362, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.938, loss: 0.217 (data_loss: 0.217, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.898, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.852, loss: 0.383 (data_loss: 0.383, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.922, loss: 0.210 (data_loss: 0.210, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.917, loss: 0.203 (data_loss: 0.203, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.881, loss: 0.325 (data_loss: 0.325, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  5\n",
      "iteration: 0, acc: 0.883, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.953, loss: 0.195 (data_loss: 0.195, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.914, loss: 0.294 (data_loss: 0.294, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.859, loss: 0.356 (data_loss: 0.356, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.914, loss: 0.197 (data_loss: 0.197, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.917, loss: 0.191 (data_loss: 0.191, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.888, loss: 0.305 (data_loss: 0.305, reg_loss: 0.000), lr: 0.001\n",
      "validation, acc: 0.875, loss: 0.387\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Dense_Layer(784,256))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(256,256))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(256,10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(accuracy=Accuracy_Classification(),loss=CrossEntropyLoss(),optimizer=Adam_Optimizer(decay_rate=1e-3))\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X,y,batch_size=128,epochs=5,print_every=100,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2b9fefb7-99d8-4f4f-b181-928de0949536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = model.forward(X_test,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "76fea8a3-e3f8-439d-8b65-5dc4b966a26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ed02a42a-7ab4-4ff1-9049-e40c1f26de7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8649"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "92728a1e-e448-410d-bb8d-d208b69bc693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = [\"T-shirt/top\",\n",
    "\"Trouser\",\n",
    "\"Pullover\",\n",
    "\"Dress\",\n",
    "\"Coat\",\n",
    "\"Sandal\",\n",
    "\"Shirt\",\n",
    "\"Sneaker\",\n",
    "\"Bag\",\n",
    "\"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ae75ef8c-204c-4fcc-93e2-f4c069994221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(images, predictions, num_rows=1, num_cols=5):\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(f\"Prediction: {predictions[i]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "521dc975-fd23-431d-8f4d-57249ae93e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_display = X_test[1:6]\n",
    "predictions_display = [results[pred[i]] for i in range(1,6)]\n",
    "plot_images_with_predictions(X_display, predictions_display)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
