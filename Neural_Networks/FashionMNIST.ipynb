{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62679da6-ddc4-4c52-a1f1-a86deb6cc345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d5113020-e73a-4a27-88c5-705c6a4e17d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1968003f-536f-4232-84f5-4e59cb5702c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000.png', '0001.png', '0002.png', '0003.png', '0004.png', '0005.png', '0006.png', '0007.png', '0008.png', '0009.png']\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('fashion_mnist_images/train/0')\n",
    "print(files[:10])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "effcd2e3-541b-450f-b18c-ed17a9a1a2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0  51   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 139 214 218 220 164 206 243 233 205  93   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 130 253 225 226 233 229 232 230 219 227 249  63   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 203 237 221 222 221 222 219 220 224 218 233 191   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 232 237 224 225 224 224 222 221 225 218 224 253   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 255 232 223 225 222 221 219 216 219 212 223 255  30   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   5 255 230 224 221 223 218 219 217 221 214 229 255  89   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  32 255 228 221 220 223 221 221 218 217 221 232 255 113   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  78 255 227 218 220 221 226 225 219 215 232 168 255 148   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 115 255 237 221 221 218 228 227 219 216 241 107 255 152   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 144 255 218 223 220 215 223 222 215 216 240 119 255 154   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 167 255 102 224 223 215 219 220 213 213 234 131 255 165   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 170 255  34 221 229 215 217 217 214 216 238 102 254 175   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 172 255  27 235 225 215 214 215 215 213 246 104 253 181   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 173 255  18 249 217 215 215 215 216 210 241  95 247 183   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 172 255  19 252 214 216 215 214 215 211 245 106 244 176   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 164 254  27 253 212 217 216 214 215 214 243 110 243 145   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 169 255  42 253 211 215 218 218 215 215 233 149 255 141   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 103 131  49 253 212 216 222 219 217 214 249 128 122  78   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  58 254 218 217 225 218 219 212 253 110   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   4   0  64 237 219 220 229 217 222 217 235 129   0   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  54 239 221 222 231 215 225 217 237 125   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  50 241 220 224 233 212 227 217 241 120   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  41 242 222 226 236 213 228 220 243 113   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  33 242 224 228 239 216 230 221 245  97   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0  13 237 224 226 235 208 226 218 246  65   0   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0 217 244 245 255 253 241 236 248  22   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 115 181 103  54 141 146 134 101   0   0   1   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)\n",
    "import cv2\n",
    "image_data = cv2.imread('fashion_mnist_images/train/3/0002.png',\n",
    "cv2.IMREAD_UNCHANGED)\n",
    "print(image_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ca6cbc82-ef7b-4c77-a9b0-b17f77b6db46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2674506cd50>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcjUlEQVR4nO3db2yV9f3/8ddpaQ9/PHRh2J5TqV01uC3WsQwc2CkWEhubjUzZkqrJAslmdAIJqcYNuWHjDWrcJGRhsmkWvrLJ5I46EolYgy0aZKkEI0HjMBapka6jSlvackrbz+8G8eR3+P/5cM5597TPR3Il9DrXu9e7V6/y6tVzrveJOOecAAAwUGDdAABg8iKEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYGaKdQPnGhsb05dffqlYLKZIJGLdDgDAk3NO/f39Ki8vV0HBpa91xl0Iffnll6qoqLBuAwBwlTo7OzVnzpxLbjPuQigWi1m3gAkg9Dzq7+/PcCe2fvjDHwbVffDBBxntA5PTlfwcZu05oeeee05VVVWaOnWq5s+fr3feeeeK6vgTHDIhEokELRNNYWFh0AJkwpX8TGUlhHbs2KG1a9dq/fr1OnjwoO644w7V19fr2LFj2dgdACBPZSWENm7cqF//+tf6zW9+o+9///vatGmTKioqtGXLlmzsDgCQpzIeQsPDwzpw4IDq6urS1tfV1Wnfvn3nbZ9MJtXX15e2AAAmh4yH0IkTJzQ6OqqysrK09WVlZerq6jpv++bmZpWUlKQWXhkHAJNH1l6YcO4TUs65Cz5JtW7dOvX29qaWzs7ObLUEABhnMv4S7dmzZ6uwsPC8q57u7u7zro4kKRqNKhqNZroNAEAeyPiVUHFxsebPn6+Wlpa09S0tLaqpqcn07gAAeSwrN6s2NjbqV7/6lRYsWKDbbrtNzz//vI4dO6aHH344G7sDAOSprIRQQ0ODenp69NRTT+n48eOqrq7Wrl27VFlZmY3dAQDyVMQ556yb+P/19fWppKTEug1kyR/+8Afvmp/+9KfeNVOmhP1+dblhixfy3nvvedfcfPPN3jVz5871rhkcHPSukaQvvvjCu2bTpk3eNX//+9+9a5A/ent7NXPmzEtuw1s5AADMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMMMAUwR76qmnvGvWr1/vXRMyTPNC7+J7JULeYHFkZMS7prCw0LtmbGzMu2ZgYMC7RpKmTp3qXVNaWupdU1dX513T1tbmXQMbDDAFAIxrhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzTNFGsPb2du+aG264wbvm5MmT3jWhU7RDfhxC9jU6OpqT/UyZMsW7RpKGh4e9a0J+bj/66CPvmiVLlnjXwAZTtAEA4xohBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzYdMNAUkVFRU52U9hYaF3TS4HmOZKroarhhoaGvKuWbx4cRY6QT7hSggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZBpgiWFlZmXfNf//7X++aXA7uHBsb864J6S9kP6OjoznZjyQlk0nvmqKiIu+aggL/34Ovv/5675pjx4551yA3uBICAJghhAAAZjIeQk1NTYpEImlLPB7P9G4AABNAVp4Tuvnmm/XWW2+lPg55UzIAwMSXlRCaMmUKVz8AgMvKynNCR44cUXl5uaqqqnTffffps88+u+i2yWRSfX19aQsAYHLIeAgtXLhQ27Zt0+7du/XCCy+oq6tLNTU16unpueD2zc3NKikpSS0VFRWZbgkAME5FXMhNDh4GBgZ044036vHHH1djY+N5jyeTybR7Evr6+giiPBFy6oTcJzQ0NORdE3L/iRR2L854vk8o9PnY06dPe9dcc8013jXf+c53vGsqKyu9a7hPyEZvb69mzpx5yW2yfrPqjBkzdMstt+jIkSMXfDwajSoajWa7DQDAOJT1+4SSyaQ+/vhjJRKJbO8KAJBnMh5Cjz32mNra2tTR0aF///vf+uUvf6m+vj6tWLEi07sCAOS5jP857osvvtD999+vEydO6Nprr9WiRYu0f//+oL/jAgAmtoyH0Msvv5zpT4ksCxk8GSrkCfmQYaQjIyPeNVLYCxpC9hXyNYXUhL5AI+QFDcXFxUH78vWTn/zEu4YXJoxfzI4DAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABgJutvaofx77rrrsvZvkLeHTRX73YaKvTdS32FfE2hg1xD3mgyV8chHo/nZD/IDa6EAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmmKIN/eAHP7Bu4ZJCpmgXFIT9fjU2NuZdU1RU5F0TMhE7pLeQYydJM2bM8K5pbW31rmloaPCuueGGG7xrMH5xJQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMA0yhuXPn5mxf06dP964ZHh7OQicXFjLws7i42LsmEol414QI3c+0adO8a9555x3vmpABpiEDYzF+cSUEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADANMoYqKipztq6enx7smZGBlyCDS0LqRkRHvGudcTvYTOsA0pL9rrrkmaF++Cgr43Xki4bsJADBDCAEAzHiH0N69e7Vs2TKVl5crEonotddeS3vcOaempiaVl5dr2rRpqq2t1eHDhzPVLwBgAvEOoYGBAc2bN0+bN2++4OPPPPOMNm7cqM2bN6u9vV3xeFx33XWX+vv7r7pZAMDE4v3ChPr6etXX11/wMeecNm3apPXr12v58uWSpBdffFFlZWXavn27HnrooavrFgAwoWT0OaGOjg51dXWprq4utS4ajerOO+/Uvn37LliTTCbV19eXtgAAJoeMhlBXV5ckqaysLG19WVlZ6rFzNTc3q6SkJLXk8uXCAABbWXl13Ln3JjjnLnq/wrp169Tb25taOjs7s9ESAGAcyujNqvF4XNLZK6JEIpFa393dfd7V0Tei0aii0Wgm2wAA5ImMXglVVVUpHo+rpaUltW54eFhtbW2qqanJ5K4AABOA95XQqVOn9Omnn6Y+7ujo0AcffKBZs2bp+uuv19q1a7VhwwbNnTtXc+fO1YYNGzR9+nQ98MADGW0cAJD/vEPo/fff15IlS1IfNzY2SpJWrFih//u//9Pjjz+uoaEhPfLII/r666+1cOFCvfnmm4rFYpnrGgAwIXiHUG1t7SWHG0YiETU1Nampqelq+kIOXez5umz44x//6F3zxBNPeNcUFhZ610jSmTNnvGtyNVh0bGzMuybU8PCwd02uBpjOmDEjJ/tBbjA7DgBghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABgJqPvrIr8FDpxOsRbb73lXfO73/3Ou2bq1KneNZJ0+vRp75pcTbcOmbxdUBD2e+aUKf7/NXR2dnrXXGoi/8VMnz7duwbjF1dCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzDDAFMFDLkMkk0nvmpkzZ3rXDA4OeteEGh0d9a4JGUYaUhPSmxQ2ADbkPAoZYFpcXOxdg/GLKyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmGGCKoMGYoYaGhrxrQgaYfvXVV941oXJ1/HI5wPTMmTPeNSHf2+HhYe+aXA7cRfbx3QQAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGAaaQcy5n+woZwllYWOhdEzq4M2Q4Zsjxy9UxDx2uOmWK/38N06dP964ZGRnxrkkmk941GL+4EgIAmCGEAABmvENo7969WrZsmcrLyxWJRPTaa6+lPb5y5UpFIpG0ZdGiRZnqFwAwgXiH0MDAgObNm6fNmzdfdJu7775bx48fTy27du26qiYBABOT97OP9fX1qq+vv+Q20WhU8Xg8uCkAwOSQleeEWltbVVpaqptuukkPPviguru7L7ptMplUX19f2gIAmBwyHkL19fV66aWXtGfPHj377LNqb2/X0qVLL/qyyubmZpWUlKSWioqKTLcEABinMn6fUENDQ+rf1dXVWrBggSorK/X6669r+fLl522/bt06NTY2pj7u6+sjiABgksj6zaqJREKVlZU6cuTIBR+PRqOKRqPZbgMAMA5l/T6hnp4edXZ2KpFIZHtXAIA8430ldOrUKX366aepjzs6OvTBBx9o1qxZmjVrlpqamvSLX/xCiURCR48e1RNPPKHZs2fr3nvvzWjjAID85x1C77//vpYsWZL6+Jvnc1asWKEtW7bo0KFD2rZtm06ePKlEIqElS5Zox44disVimesaADAheIdQbW3tJYcv7t69+6oaQu4NDQ0F1YUM4Zw6dap3TcgQztABpiHDUkPkaoBp6NczNjbmXVNUVBS0L1+ff/55TvaD3GB2HADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADATNbfWRXjX3FxcVBdyHTrkpKSoH3lSsj06JBJ1QUF/r//hUwGD/keSdLg4KB3zXe/+13vmmuuuca7ZmRkxLsG4xdXQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwwwBTas2dPUN1//vMf75rQYam5EjLwc8oU/x+jkGGkuRp6Kknf+ta3vGv+97//edc88cQT3jWHDx/2rsH4xZUQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAMwwwhf7617/mbF81NTXeNSFDRaPRqHeNJDnnvGsKCwu9a0ZGRrxrQgaYDg8Pe9dIYUNZv/rqK++aP/3pT941mFi4EgIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGAaYIGlYphQ3hLC4u9q75+uuvvWtChp5KYUNCQ47f6dOnc7KfkOMthQ0j/fa3vx20L18hA2NHR0ez0AkygSshAIAZQggAYMYrhJqbm3XrrbcqFouptLRU99xzjz755JO0bZxzampqUnl5uaZNm6ba2lodPnw4o00DACYGrxBqa2vTqlWrtH//frW0tGhkZER1dXUaGBhIbfPMM89o48aN2rx5s9rb2xWPx3XXXXepv78/480DAPKb1zOdb7zxRtrHW7duVWlpqQ4cOKDFixfLOadNmzZp/fr1Wr58uSTpxRdfVFlZmbZv366HHnooc50DAPLeVT0n1NvbK0maNWuWJKmjo0NdXV2qq6tLbRONRnXnnXdq3759F/wcyWRSfX19aQsAYHIIDiHnnBobG3X77berurpaktTV1SVJKisrS9u2rKws9di5mpubVVJSkloqKipCWwIA5JngEFq9erU+/PBD/fOf/zzvsXPv0XDOXfS+jXXr1qm3tze1dHZ2hrYEAMgzQXcprlmzRjt37tTevXs1Z86c1Pp4PC7p7BVRIpFIre/u7j7v6ugb0WhU0Wg0pA0AQJ7zuhJyzmn16tV65ZVXtGfPHlVVVaU9XlVVpXg8rpaWltS64eFhtbW1qaamJjMdAwAmDK8roVWrVmn79u3617/+pVgslnqep6SkRNOmTVMkEtHatWu1YcMGzZ07V3PnztWGDRs0ffp0PfDAA1n5AgAA+csrhLZs2SJJqq2tTVu/detWrVy5UpL0+OOPa2hoSI888oi+/vprLVy4UG+++aZisVhGGgYATBxeIeScu+w2kUhETU1NampqCu0JOXYl39dMGRwc9K4JGSoaUiOFDT4N2dfY2Jh3TcjgzjNnznjXSGFfU8hA2xC5PF+RfcyOAwCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYCXpnVSBUyKTlkInOoZOWQyZV52ridOhk8BBTpvj/1xA6sRuTG1dCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzDDAFDk1ODjoXROJRLxrQod9htTlauhpiJDepLD+kslk0L4wuXElBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwDTJFTp0+fzsl+QgeEhgxLDRkSmquakK9HCjt+J0+eDNoXJjeuhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJhhgClyqqenx7vmzJkzWejkwpxz3jUjIyPeNSGDXKdPn+5dMzo66l0jhQ0+PXr0aNC+fIUOZcX4xJUQAMAMIQQAMOMVQs3Nzbr11lsVi8VUWlqqe+65R5988knaNitXrlQkEklbFi1alNGmAQATg1cItbW1adWqVdq/f79aWlo0MjKiuro6DQwMpG1399136/jx46ll165dGW0aADAxeL0w4Y033kj7eOvWrSotLdWBAwe0ePHi1PpoNKp4PJ6ZDgEAE9ZVPSfU29srSZo1a1ba+tbWVpWWluqmm27Sgw8+qO7u7ot+jmQyqb6+vrQFADA5BIeQc06NjY26/fbbVV1dnVpfX1+vl156SXv27NGzzz6r9vZ2LV26VMlk8oKfp7m5WSUlJamloqIitCUAQJ4Jvk9o9erV+vDDD/Xuu++mrW9oaEj9u7q6WgsWLFBlZaVef/11LV++/LzPs27dOjU2NqY+7uvrI4gAYJIICqE1a9Zo586d2rt3r+bMmXPJbROJhCorK3XkyJELPh6NRhWNRkPaAADkOa8Qcs5pzZo1evXVV9Xa2qqqqqrL1vT09Kizs1OJRCK4SQDAxOT1nNCqVav0j3/8Q9u3b1csFlNXV5e6uro0NDQkSTp16pQee+wxvffeezp69KhaW1u1bNkyzZ49W/fee29WvgAAQP7yuhLasmWLJKm2tjZt/datW7Vy5UoVFhbq0KFD2rZtm06ePKlEIqElS5Zox44disViGWsaADAxeP857lKmTZum3bt3X1VDAIDJgyna0NjYWM721d/f711z4sQJ75ry8nLvGkkqKPC/ayFkuvW599ZdiVOnTnnXhPQmhR2Hw4cPB+3LVy7PV2QfA0wBAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYYYApLjsd3drzzz/vXbN06dKgfe3cudO75uDBg941DQ0N3jVFRUXeNSFDTyWpo6PDu6a7uztoX5jcuBICAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgJlxNztuvM8xQ+4lk0nvmsHBwaB9DQ8Pe9eMjo5615w+fdq7ZmRkJCf7kcKOQ67wf0T+uJLvVcSNs+/oF198oYqKCus2AABXqbOzU3PmzLnkNuMuhMbGxvTll18qFospEomkPdbX16eKigp1dnZq5syZRh3a4zicxXE4i+NwFsfhrPFwHJxz6u/vV3l5uQoKLv2sz7j7c1xBQcFlk3PmzJmT+iT7BsfhLI7DWRyHszgOZ1kfh5KSkivajhcmAADMEEIAADN5FULRaFRPPvmkotGodSumOA5ncRzO4jicxXE4K9+Ow7h7YQIAYPLIqyshAMDEQggBAMwQQgAAM4QQAMBMXoXQc889p6qqKk2dOlXz58/XO++8Y91STjU1NSkSiaQt8Xjcuq2s27t3r5YtW6by8nJFIhG99tpraY8759TU1KTy8nJNmzZNtbW1Onz4sE2zWXS547By5crzzo9FixbZNJslzc3NuvXWWxWLxVRaWqp77rlHn3zySdo2k+F8uJLjkC/nQ96E0I4dO7R27VqtX79eBw8e1B133KH6+nodO3bMurWcuvnmm3X8+PHUcujQIeuWsm5gYEDz5s3T5s2bL/j4M888o40bN2rz5s1qb29XPB7XXXfdpf7+/hx3ml2XOw6SdPfdd6edH7t27cphh9nX1tamVatWaf/+/WppadHIyIjq6uo0MDCQ2mYynA9XchykPDkfXJ748Y9/7B5++OG0dd/73vfc73//e6OOcu/JJ5908+bNs27DlCT36quvpj4eGxtz8XjcPf3006l1p0+fdiUlJe4vf/mLQYe5ce5xcM65FStWuJ///Ocm/Vjp7u52klxbW5tzbvKeD+ceB+fy53zIiyuh4eFhHThwQHV1dWnr6+rqtG/fPqOubBw5ckTl5eWqqqrSfffdp88++8y6JVMdHR3q6upKOzei0ajuvPPOSXduSFJra6tKS0t100036cEHH1R3d7d1S1nV29srSZo1a5akyXs+nHscvpEP50NehNCJEyc0OjqqsrKytPVlZWXq6uoy6ir3Fi5cqG3btmn37t164YUX1NXVpZqaGvX09Fi3Zuab7/9kPzckqb6+Xi+99JL27NmjZ599Vu3t7Vq6dGnQ+zHlA+ecGhsbdfvtt6u6ulrS5DwfLnQcpPw5H8bdFO1LOfetHZxz562byOrr61P/vuWWW3Tbbbfpxhtv1IsvvqjGxkbDzuxN9nNDkhoaGlL/rq6u1oIFC1RZWanXX39dy5cvN+wsO1avXq0PP/xQ77777nmPTabz4WLHIV/Oh7y4Epo9e7YKCwvP+02mu7v7vN94JpMZM2bolltu0ZEjR6xbMfPNqwM5N86XSCRUWVk5Ic+PNWvWaOfOnXr77bfT3vplsp0PFzsOFzJez4e8CKHi4mLNnz9fLS0taetbWlpUU1Nj1JW9ZDKpjz/+WIlEwroVM1VVVYrH42nnxvDwsNra2ib1uSFJPT096uzsnFDnh3NOq1ev1iuvvKI9e/aoqqoq7fHJcj5c7jhcyLg9HwxfFOHl5ZdfdkVFRe5vf/ub++ijj9zatWvdjBkz3NGjR61by5lHH33Utba2us8++8zt37/f/exnP3OxWGzCH4P+/n538OBBd/DgQSfJbdy40R08eNB9/vnnzjnnnn76aVdSUuJeeeUVd+jQIXf//fe7RCLh+vr6jDvPrEsdh/7+fvfoo4+6ffv2uY6ODvf222+72267zV133XUT6jj89re/dSUlJa61tdUdP348tQwODqa2mQznw+WOQz6dD3kTQs459+c//9lVVla64uJi96Mf/Sjt5YiTQUNDg0skEq6oqMiVl5e75cuXu8OHD1u3lXVvv/22k3TesmLFCufc2ZflPvnkky4ej7toNOoWL17sDh06ZNt0FlzqOAwODrq6ujp37bXXuqKiInf99de7FStWuGPHjlm3nVEX+volua1bt6a2mQznw+WOQz6dD7yVAwDATF48JwQAmJgIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCY+X8a58xxBWWleAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "919bafd6-b8da-44d3-a061-d091dd49dd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = os.listdir('fashion_mnist_images/train/')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec70b22d-d4db-491b-823e-7516e232e5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(dataset,path):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label in tqdm(labels):\n",
    "        for file in os.listdir(os.path.join(path,dataset,label)):\n",
    "            if(file[0] == '.'):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(path,dataset,label,file),cv2.IMREAD_UNCHANGED)\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).astype('uint8')\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e0a926e5-0a0f-481c-a696-1232ba49a46e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"fashion_mnist_images/\"\n",
    "\n",
    "X,y = load_data(\"train\",PATH)\n",
    "X_test,y_test = load_data(\"test\",PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333db112-6606-49c0-9958-57fa70076166",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bb8491ca-84e3-43f5-98b6-cef135759cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling the data between -1 and 1\n",
    "\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "496c1054-3360-4649-add0-dee0bfb0e82f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(),X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d374a650-fa55-43ae-921b-f20f6032db17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (60000, 28, 28)  X_test shape :  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape : \",X.shape,\" X_test shape : \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c1d0e7b8-9c6b-4750-bd9d-7160dd00f912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (60000, 784)  X_test shape :  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Converting the 2D array of features into a 1D array\n",
    "X = X.reshape(X.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "print(\"X shape : \",X.shape,\" X_test shape : \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d7c482c0-c4cd-4cca-b802-401c66acaf05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "\n",
    "idxs = np.array(range(X.shape[0]))\n",
    "\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "X = X[idxs]\n",
    "y = y[idxs]\n",
    "\n",
    "test_idxs = np.array(range(X_test.shape[0]))\n",
    "np.random.shuffle(test_idxs)\n",
    "\n",
    "X_test = X_test[test_idxs]\n",
    "y_test = y_test[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d7094c3b-a40b-4b0b-924d-bb455482c4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 0, 2, 7, 8, 0, 3, 5, 0], dtype=uint8)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "39bb7359-fb5c-405e-b2f1-ce26fe9dc8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self,ninputs,nneurons,l1_w=0,l1_b=0,l2_w=0,l2_b=0):\n",
    "        # Initialising weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(ninputs,nneurons)\n",
    "        self.biases = np.zeros((1,nneurons))\n",
    "        # Regularization\n",
    "        self.l1_w = l1_w\n",
    "        self.l1_b = l1_b\n",
    "        self.l2_w = l2_w\n",
    "        self.l2_b = l2_b\n",
    "        \n",
    "    # Forward Propagation    \n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    # Backpropagation\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        \n",
    "        if self.l1_w > 0:\n",
    "            dl1w = np.ones_like(self.weights)\n",
    "            dl1w[self.weights < 0] = -1\n",
    "            self.dweights += dl1w\n",
    "        if self.l1_b > 0:\n",
    "            dl1b = np.ones_like(self.biases)\n",
    "            dl1b[self.biases < 0] = -1\n",
    "            self.dbiases += sl1b\n",
    "        if self.l2_w > 0:\n",
    "            self.dweights += self.weights * 2 * self.l2_w\n",
    "        if self.l2_b > 0:\n",
    "            self.dbiases += self.biases * 2 * self.l2_b\n",
    "            \n",
    "\n",
    "class Dropout_layer:\n",
    "    def __init__(self,drop_rate=0):\n",
    "        self.drop_rate = 1 - drop_rate\n",
    "    def forward(self,inputs,training):\n",
    "        if not training:\n",
    "            self.output = input.copy()\n",
    "            return\n",
    "        self.dropmask = np.random.binomial(1,self.drop_rate,size =inputs.shape)/(self.drop_rate)\n",
    "        self.output = inputs * self.dropmask\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.dropmask\n",
    "        \n",
    "class Activation_Relu:\n",
    "    def forward(self,inputs,training):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self,inputs,training):\n",
    "        self.inputs= inputs\n",
    "        self.outputs = 1/(1+np.exp(-inputs))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues * self.outputs * (1 - self.outputs)\n",
    "        \n",
    "class Loss_BinaryCrossentropy():\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "        (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs,y_true):\n",
    "        expvals = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True) )\n",
    "        self.output = expvals/np.sum(expvals,axis=1,keepdims=True)\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range(samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def calculate_accumulated(self,*,regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        return data_loss,self.regularization_loss()\n",
    "    \n",
    "    def calculate(self, output, y, *, regularization=False):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not regularization:\n",
    "            return data_loss\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "        \n",
    "        return np.mean(-np.log(y_pred[range(len(y_pred)),y_true]))\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_w > 0:\n",
    "                loss += layer.l1_w * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_b > 0:\n",
    "                loss += layer.l1_b * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_w > 0:\n",
    "                loss += layer.l2_w * np.sum(layer.weights * layer.weights)\n",
    "            if layer.l2_b > 0:\n",
    "                loss += layer.l2_b * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "        \n",
    "        \n",
    "class Activation_softmax_cross_entropy:\n",
    "#     def __init__(self):\n",
    "#         self.activation = Activation_Softmax()\n",
    "#         self.lossfunc = CrossEntropyLoss()\n",
    "    \n",
    "#     def forward(self,inputs,y_true):\n",
    "#         self.activation.forward(inputs,y_true)\n",
    "#         self.output = self.activation.output\n",
    "#         return self.lossfunc.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(y_true)\n",
    "        \n",
    "        # Turning one hot encoded arrays to sparse vectors\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        \n",
    "        self.dinputs/=samples\n",
    "        \n",
    "        \n",
    "class Adam_Optimizer:\n",
    "    def __init__(self,lr=0.001,decay_rate=0,epsilon= 1e-7,beta1=0.9,beta2=0.999):\n",
    "        self.initiallr = lr\n",
    "        self.currentlr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "    \n",
    "    def pre_update(self):\n",
    "        self.currentlr = self.initiallr * (1/(1+(self.decay_rate * self.iterations)))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentum = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_momentum = self.beta1 * layer.weight_momentum + (1-self.beta1) * layer.dweights\n",
    "        layer.bias_momentum = self.beta1 * layer.bias_momentum + (1 - self.beta1) * layer.dbiases\n",
    "        \n",
    "        layer.weight_momentum_prime = layer.weight_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        layer.bias_momentum_prime = layer.bias_momentum / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = layer.weight_cache * self.beta2 + (1-self.beta2) * (layer.dweights ** 2)\n",
    "        layer.bias_cache = layer.bias_cache * self.beta2 + (1-self.beta2) * (layer.dbiases ** 2)\n",
    "        \n",
    "        layer.weight_cache_prime = layer.weight_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "        layer.bias_cache_prime = layer.bias_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += - self.currentlr * layer.weight_momentum_prime  / (np.sqrt(layer.weight_cache_prime) + self.epsilon)\n",
    "        layer.biases += -self.currentlr * layer.bias_momentum_prime / (np.sqrt(layer.bias_cache_prime) + self.epsilon)\n",
    "        \n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "    \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self,*,loss,optimizer,accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "    \n",
    "    def train(self,X,y,*,epochs,print_every,validation_data = None,batch_size = None):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "            X_val,y_val = validation_data\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "                \n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            \n",
    "            print(\"Epoch : \",epoch)\n",
    "            \n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size : (step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size : (step+1) * batch_size]\n",
    "\n",
    "                    \n",
    "                output = self.forward(batch_X,training = True)\n",
    "                data_loss,regularization_loss = self.loss.calculate(output,batch_y,regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions,batch_y)\n",
    "\n",
    "                self.backward(output,batch_y)\n",
    "\n",
    "                self.optimizer.pre_update()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'iteration: {step}, ' +\n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss: {loss:.3f} (' +\n",
    "                    f'data_loss: {data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "            \n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            \n",
    "            print(f'training, ' +\n",
    "                    f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                    f'loss: {epoch_loss:.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                    f'lr: {self.optimizer.currentlr}')\n",
    "\n",
    "            \n",
    "            \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.accuracy.new_pass()\n",
    "            self.loss.new_pass()\n",
    "            \n",
    "            for step in range(validation_steps):\n",
    "                if batch_size is None:\n",
    "                    batch_X = X_val\n",
    "                    batch_y = y_val\n",
    "                else:\n",
    "                    batch_X = X_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    batch_y = y_val[step * batch_size : (step + 1) * batch_size]\n",
    "                    \n",
    "            \n",
    "            output = self.forward(batch_X,training=False)\n",
    "            loss = self.loss.calculate(output, batch_y)\n",
    "            predictions = self.output_activation.predictions(\n",
    "            output)\n",
    "            accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "            validation_loss = self.loss.calculate_accumulated()\n",
    "            print(f'validation, ' +\n",
    "            f'acc: {validation_accuracy:.3f}, ' +\n",
    "            f'loss: {validation_loss:.3f}')\n",
    "\n",
    "        \n",
    "    def finalize(self):\n",
    "        self.input_layer = Input_Layer()\n",
    "        self.trainable_layers = []\n",
    "        nlayers = len(self.layers)\n",
    "        \n",
    "        for i in range(nlayers):\n",
    "            if i==0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            elif i < nlayers - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_activation = self.layers[i]\n",
    "            \n",
    "            if hasattr(self.layers[i],\"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)   \n",
    "        \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "        isinstance(self.loss, CrossEntropyLoss):\n",
    "            self.softmax_classifier_output = \\\n",
    "            Activation_softmax_cross_entropy()\n",
    "\n",
    "            \n",
    "    def forward(self,X,training):\n",
    "        self.input_layer.forward(X)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output,training)\n",
    "        return layer.output\n",
    "    def backward(self,output,y):\n",
    "        \n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = \\\n",
    "            self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "        \n",
    "        \n",
    "        self.loss.backward(output,y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "        \n",
    "class Input_Layer:\n",
    "    def forward(self,inputs):\n",
    "        self.output = inputs\n",
    "        \n",
    "class Accuracy:\n",
    "    def calculate(self,predictions,y):\n",
    "        comparisions = self.compare(predictions,y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "        self.accumulated_sum += accuracy\n",
    "        self.accumulated_count += 1\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_count = 0\n",
    "        self.accumulated_sum = 0\n",
    "    \n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "class Accuracy_Classification(Accuracy):\n",
    "    def init(self,y):\n",
    "        pass\n",
    "    def compare(self,predictions,y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y,axis=1)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b31e6ab7-5c23-48ee-b16d-560498857e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "iteration: 0, acc: 0.078, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.773, loss: 0.549 (data_loss: 0.549, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.812, loss: 0.537 (data_loss: 0.537, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.820, loss: 0.521 (data_loss: 0.521, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.828, loss: 0.368 (data_loss: 0.368, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.885, loss: 0.298 (data_loss: 0.298, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.794, loss: 0.555 (data_loss: 0.555, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  2\n",
      "iteration: 0, acc: 0.844, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.914, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.852, loss: 0.408 (data_loss: 0.408, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.852, loss: 0.449 (data_loss: 0.449, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.906, loss: 0.269 (data_loss: 0.269, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.896, loss: 0.244 (data_loss: 0.244, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.856, loss: 0.390 (data_loss: 0.390, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  3\n",
      "iteration: 0, acc: 0.875, loss: 0.375 (data_loss: 0.375, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.930, loss: 0.253 (data_loss: 0.253, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.875, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.867, loss: 0.410 (data_loss: 0.410, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.922, loss: 0.229 (data_loss: 0.229, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.906, loss: 0.218 (data_loss: 0.218, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.872, loss: 0.350 (data_loss: 0.350, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  4\n",
      "iteration: 0, acc: 0.883, loss: 0.362 (data_loss: 0.362, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.938, loss: 0.217 (data_loss: 0.217, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.898, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.852, loss: 0.383 (data_loss: 0.383, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.922, loss: 0.210 (data_loss: 0.210, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.917, loss: 0.203 (data_loss: 0.203, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.881, loss: 0.325 (data_loss: 0.325, reg_loss: 0.000), lr: 0.001\n",
      "Epoch :  5\n",
      "iteration: 0, acc: 0.883, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 100, acc: 0.953, loss: 0.195 (data_loss: 0.195, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 200, acc: 0.914, loss: 0.294 (data_loss: 0.294, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 300, acc: 0.859, loss: 0.356 (data_loss: 0.356, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 400, acc: 0.914, loss: 0.197 (data_loss: 0.197, reg_loss: 0.000), lr: 0.001\n",
      "iteration: 468, acc: 0.917, loss: 0.191 (data_loss: 0.191, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 0.888, loss: 0.305 (data_loss: 0.305, reg_loss: 0.000), lr: 0.001\n",
      "validation, acc: 0.875, loss: 0.387\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Dense_Layer(784,256))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(256,256))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Dense_Layer(256,10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(accuracy=Accuracy_Classification(),loss=CrossEntropyLoss(),optimizer=Adam_Optimizer(decay_rate=1e-3))\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X,y,batch_size=128,epochs=5,print_every=100,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2b9fefb7-99d8-4f4f-b181-928de0949536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = model.forward(X_test,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "76fea8a3-e3f8-439d-8b65-5dc4b966a26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ed02a42a-7ab4-4ff1-9049-e40c1f26de7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8649"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "92728a1e-e448-410d-bb8d-d208b69bc693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = [\"T-shirt/top\",\n",
    "\"Trouser\",\n",
    "\"Pullover\",\n",
    "\"Dress\",\n",
    "\"Coat\",\n",
    "\"Sandal\",\n",
    "\"Shirt\",\n",
    "\"Sneaker\",\n",
    "\"Bag\",\n",
    "\"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ae75ef8c-204c-4fcc-93e2-f4c069994221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(images, predictions, num_rows=1, num_cols=5):\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(f\"Prediction: {predictions[i]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "521dc975-fd23-431d-8f4d-57249ae93e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_display = X_test[1:6]\n",
    "predictions_display = [results[pred[i]] for i in range(1,6)]\n",
    "plot_images_with_predictions(X_display, predictions_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc66b1b-9cd2-4cc8-8c4b-bac766d0986f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
